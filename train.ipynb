{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy as dc\n",
    "from game import Game\n",
    "from heuristics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetworkConnect4(nn.Module):\n",
    "    # def __init__(self, env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(1, 10, kernel_size=4, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(10*3*4, 42)\n",
    "        self.fc2 = nn.Linear(42, 20)\n",
    "        self.fc3 = nn.Linear(20, 7)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(0)  # Add an extra dimension for the channels\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.view(-1, 10*3*4)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_frames):\n",
    "        self.max_frames = max_frames\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, frame):\n",
    "        self.buffer.append(frame)\n",
    "        if len(self.buffer) > self.max_frames:\n",
    "            del self.buffer[0:len(self.buffer)-self.max_frames]\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        # Ensure we don't pick the same frame twice\n",
    "        # Record the random indices picked from elements in the buffer\n",
    "        sample_nums = set()\n",
    "        while len(sample_nums) < num_samples:\n",
    "            sample_nums.add(random.randrange(len(self.buffer)))\n",
    "        experiences = [self.buffer[i] for i in sample_nums]\n",
    "        return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(step, epsilon_start, epsilon_finish, total_timesteps, exploration_fraction):\n",
    "    finish_step = total_timesteps * exploration_fraction\n",
    "    if step > finish_step:\n",
    "        return epsilon_finish\n",
    "    epsilon_range = epsilon_start - epsilon_finish\n",
    "    return epsilon_finish + (((finish_step - step) / finish_step) * epsilon_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "seed = 0\n",
    "buffer_size = 10000\n",
    "learning_rate = 2e-3 # should be lower\n",
    "ideal_batch_size = 1000\n",
    "total_timesteps = 200\n",
    "train_games = 20\n",
    "epsilon_start = 1\n",
    "epsilon_finish = 0\n",
    "exploration_fraction = 0.95\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = DeepQNetworkConnect4()\n",
    "old_q_network = DeepQNetworkConnect4()\n",
    "buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLoss(experiences, q_network = None, target_network = None):\n",
    "    # mean squared of (r + gamma * max_a Q(s', a)) - Q(s, a)\n",
    "    loss = 0\n",
    "    for i in experiences:\n",
    "        state, action, reward, next_state = i.state, i.action, i.reward, i.next_state\n",
    "        # change state shape to (6, 7)\n",
    "        state = np.array(state).reshape(1, 6, 7)\n",
    "        if reward == None:\n",
    "            next_state = np.array(next_state).reshape(1, 6, 7)\n",
    "        state_tensor = torch.from_numpy(state).float()\n",
    "        if reward == None:\n",
    "            next_state_tensor = torch.from_numpy(next_state).float()\n",
    "            loss += (gamma * torch.max(target_network(next_state_tensor)) - q_network(state_tensor)[action]) ** 2\n",
    "        if next_state is None:\n",
    "            # huber loss instead of MSE\n",
    "            loss += (reward - q_network(state_tensor)[action]) ** 2\n",
    "    return loss\n",
    "\n",
    "def train(q_network, opponent, epsilon_initial_value = 0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True if seed > 0 else False\n",
    "\n",
    "    # Initialize action-value function Q and target network\n",
    "    # TODO: Add extra agents to play against\n",
    "    target_network = dc(q_network)\n",
    "    optimiser = torch.optim.Adam(q_network.parameters(), learning_rate)\n",
    "\n",
    "    epsilon_start = epsilon_initial_value\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    for iter in range(int(total_timesteps)):\n",
    "        for step in range(int(train_games)):\n",
    "            epsilon = calculate_epsilon(iter, epsilon_start, epsilon_finish, total_timesteps, exploration_fraction)\n",
    "            # Generate games and add experiences\n",
    "            g = Game()\n",
    "            # TODO: Why is the first player dominating? first_player is random, should be 50/50\n",
    "            g.playGame(agent1 = q_network, agent2 = opponent, epsilon = epsilon, first_player = random.randint(1, 2))\n",
    "            rewards.append(g.state)\n",
    "            for experience in g.experiences:\n",
    "                buffer.add(experience)\n",
    "\n",
    "        # print(sum(rewards[-20:-1]))\n",
    "\n",
    "        batch = buffer.sample(min(len(buffer.buffer), ideal_batch_size))\n",
    "        # states and next states should be floats (same as the OUTPUT)\n",
    "        # brackets are required to turn generator into a list\n",
    "\n",
    "        # get loss\n",
    "        loss = QLoss(batch, q_network, target_network)\n",
    "        losses.append(loss.item())\n",
    "        if iter % 10 == 0:\n",
    "            print(iter, loss, epsilon, sum(rewards)/len(rewards))\n",
    "            # comment out if no need\n",
    "            torch.save(q_network, 'q_conv_network.pth')\n",
    "            \n",
    "        '''\n",
    "        if len(rewards) > 500:\n",
    "            print(iter, loss, epsilon, sum(rewards[-500:])/500)\n",
    "        else:\n",
    "            print(iter, loss, epsilon, sum(rewards)/len(rewards))\n",
    "        '''\n",
    "        # backprop\n",
    "        with torch.no_grad():\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "    # graph losses\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    return q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VS Random\n",
    "q_networks = []\n",
    "total_timesteps = 2000\n",
    "q_network = train(q_network, 1, 0.9)\n",
    "q_networks.append(q_network)\n",
    "torch.save(q_network, 'q_conv_network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VS Minimax\n",
    "q_network = torch.load('q_conv_network.pth')\n",
    "\n",
    "# 1000 timesteps done, 1000 left to go\n",
    "# fix based on how far training got\n",
    "total_timesteps = 100\n",
    "\n",
    "q_network = train(q_network, 3, 0.9)\n",
    "q_networks.append(q_network)\n",
    "torch.save(q_network, 'q_conv_network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VS Itself\n",
    "q_network = torch.load('q_conv_network.pth')\n",
    "\n",
    "# 1000 timesteps done, 0 left to go\n",
    "# fix based on how far training got\n",
    "total_timesteps = 500\n",
    "old_q_network = dc(q_network)\n",
    "q_network = train(q_network, old_q_network, 0.9)\n",
    "q_networks.append(q_network)\n",
    "torch.save(q_network, 'q_conv_network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game()\n",
    "g.playGame(agent1 = q_network, agent2 = 3, epsilon = 0, first_player = 1, pick_display = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
