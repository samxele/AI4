{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy as dc\n",
    "from game import Game\n",
    "from heuristics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetworkConnect4(nn.Module):\n",
    "    # def __init__(self, env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(42, 42),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(42, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 7),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_frames):\n",
    "        self.max_frames = max_frames\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, frame):\n",
    "        self.buffer.append(frame)\n",
    "        if len(self.buffer) > self.max_frames:\n",
    "            del self.buffer[0:len(self.buffer)-self.max_frames]\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        # Ensure we don't pick the same frame twice\n",
    "        # Record the random indices picked from elements in the buffer\n",
    "        sample_nums = set()\n",
    "        while len(sample_nums) < num_samples:\n",
    "            sample_nums.add(random.randrange(len(self.buffer)))\n",
    "        experiences = [self.buffer[i] for i in sample_nums]\n",
    "        return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(step, epsilon_start, epsilon_finish, total_timesteps, exploration_fraction):\n",
    "    finish_step = total_timesteps * exploration_fraction\n",
    "    if step > finish_step:\n",
    "        return epsilon_finish\n",
    "    epsilon_range = epsilon_start - epsilon_finish\n",
    "    return epsilon_finish + (((finish_step - step) / finish_step) * epsilon_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "seed = 0\n",
    "buffer_size = 10000\n",
    "learning_rate = 2e-3 # should be lower\n",
    "ideal_batch_size = 1000\n",
    "total_timesteps = 200\n",
    "train_games = 20\n",
    "epsilon_start = 0.9\n",
    "epsilon_finish = 0\n",
    "exploration_fraction = 0.8\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = DeepQNetworkConnect4()\n",
    "old_q_network = DeepQNetworkConnect4()\n",
    "buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLoss(experiences, q_network, target_network):\n",
    "    loss = 0\n",
    "\n",
    "    for exp in experiences:\n",
    "        state, action, reward, next_state = exp.state, exp.action, exp.reward, exp.next_state\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = torch.flatten(state, start_dim = 0, end_dim = 1) # double check dimensions\n",
    "\n",
    "        if reward == None:\n",
    "            next_state = torch.from_numpy(next_state).float()\n",
    "            next_state = torch.flatten(next_state, start_dim = 0, end_dim = 1) # double check dimensions\n",
    "            loss += (gamma * torch.max(target_network(next_state)) - q_network(state)[action]) ** 2\n",
    "        else: # next_state is None\n",
    "            loss += (reward - q_network(state)[action]) ** 2\n",
    "\n",
    "    loss /= len(experiences)\n",
    "    return loss\n",
    "\n",
    "def train(q_network, opponent, epsilon_initial_value = 0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True if seed > 0 else False\n",
    "\n",
    "    # Initialize action-value function Q and target network\n",
    "    # TODO: Add extra agents to play against\n",
    "    target_network = dc(q_network)\n",
    "    optimiser = torch.optim.Adam(q_network.parameters(), learning_rate)\n",
    "\n",
    "    epsilon_start = epsilon_initial_value\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    for iter in range(int(total_timesteps)):\n",
    "        for step in range(int(train_games)):\n",
    "            epsilon = calculate_epsilon(iter, epsilon_start, epsilon_finish, total_timesteps, exploration_fraction)\n",
    "            # Generate games and add experiences\n",
    "            g = Game()\n",
    "            # TODO: Why is the first player dominating? first_player is random, should be 50/50\n",
    "            g.playGame(agent1 = q_network, agent2 = opponent, epsilon = epsilon, first_player = random.randint(1, 2))\n",
    "            rewards.append(g.state)\n",
    "            for experience in g.experiences:\n",
    "                buffer.add(experience)\n",
    "\n",
    "        # print(sum(rewards[-20:-1]))\n",
    "\n",
    "        batch = buffer.sample(min(len(buffer.buffer), ideal_batch_size))\n",
    "        # states and next states should be floats (same as the OUTPUT)\n",
    "        # brackets are required to turn generator into a list\n",
    "\n",
    "        # get loss\n",
    "        loss = QLoss(batch, q_network, target_network)\n",
    "        losses.append(loss.item())\n",
    "        if iter % 20 == 0:\n",
    "            print(iter, loss, epsilon, sum(rewards)/len(rewards))\n",
    "        '''\n",
    "        if len(rewards) > 500:\n",
    "            print(iter, loss, epsilon, sum(rewards[-500:])/500)\n",
    "        else:\n",
    "            print(iter, loss, epsilon, sum(rewards)/len(rewards))\n",
    "        '''\n",
    "        # backprop\n",
    "        with torch.no_grad():\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "    # graph losses\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    return q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.1446, grad_fn=<DivBackward0>) 0.9 0.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m q_networks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m q_network \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m q_networks\u001b[38;5;241m.\u001b[39mappend(q_network)\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(q_network, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_network.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(q_network, opponent, epsilon_initial_value)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     64\u001b[0m         optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 65\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m         optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# graph losses\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_networks = []\n",
    "q_network = train(q_network, 1, 0.9)\n",
    "q_networks.append(q_network)\n",
    "torch.save(q_network, 'q_network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for i in range(7): # Fix if crash\n",
    "    \n",
    "    # load q_network\n",
    "    q_network = torch.load('q_network.pth')\n",
    "\n",
    "    total_timesteps = 200\n",
    "    old_q_network = dc(q_network)\n",
    "    q_network = train(q_network, old_q_network, 0.9)\n",
    "    q_networks.append(q_network)\n",
    "    torch.save(q_network, 'q_network.pth')\n",
    "\n",
    "    # load q_network\n",
    "    q_network = torch.load('q_network.pth')\n",
    "\n",
    "    total_timesteps = 400\n",
    "    q_network = train(q_network, 3, 0.9)\n",
    "    q_networks.append(q_network)\n",
    "    torch.save(q_network, 'q_network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game()\n",
    "g.playGame(agent1 = q_network, agent2 = 3, epsilon = 0.9, first_player = 1, pick_display = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
