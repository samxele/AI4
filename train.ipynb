{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from game import Game\n",
    "from heuristics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetworkConnect4(nn.Module):\n",
    "    # def __init__(self, env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(42, 42),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(42, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 7),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, max_frames):\n",
    "        self.max_frames = max_frames\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, frame):\n",
    "        self.buffer.append(frame)\n",
    "        if len(self.buffer) > self.max_frames:\n",
    "            del self.buffer[0:len(self.buffer)-self.max_frames]\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        # Ensure we don't pick the same frame twice\n",
    "        # Record the random indices picked from elements in the buffer\n",
    "        sample_nums = set()\n",
    "        while len(sample_nums) < num_samples:\n",
    "            sample_nums.add(random.randrange(len(self.buffer)))\n",
    "        experiences = [self.buffer[i] for i in sample_nums]\n",
    "        return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(step, epsilon_start, epsilon_finish, total_timesteps, exploration_fraction):\n",
    "    finish_step = total_timesteps * exploration_fraction\n",
    "    if step > finish_step:\n",
    "        return epsilon_finish\n",
    "    epsilon_range = epsilon_start - epsilon_finish\n",
    "    return epsilon_finish + (((finish_step - step) / finish_step) * epsilon_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "seed = 0\n",
    "buffer_size = 10000\n",
    "learning_rate = 2.5e-2 # should be lower\n",
    "ideal_batch_size = 1000\n",
    "total_timesteps = 100\n",
    "train_games = 20\n",
    "epsilon_start = 0.9\n",
    "epsilon_finish = 0\n",
    "exploration_fraction = 0.8\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0 tensor(0.1140, grad_fn=<DivBackward0>) 0.9 0.1\n",
      "9\n",
      "1 tensor(0.0876, grad_fn=<DivBackward0>) 0.88875 0.25\n",
      "-9\n",
      "2 tensor(0.0923, grad_fn=<DivBackward0>) 0.8775 0.03333333333333333\n",
      "-1\n",
      "3 tensor(0.0919, grad_fn=<DivBackward0>) 0.8662500000000001 0.0\n",
      "1\n",
      "4 tensor(0.0903, grad_fn=<DivBackward0>) 0.855 0.0\n",
      "-1\n",
      "5 tensor(0.0893, grad_fn=<DivBackward0>) 0.84375 -0.016666666666666666\n",
      "-3\n",
      "6 tensor(0.0931, grad_fn=<DivBackward0>) 0.8325 -0.02857142857142857\n",
      "7\n",
      "7 tensor(0.0902, grad_fn=<DivBackward0>) 0.82125 0.0125\n",
      "1\n",
      "8 tensor(0.0960, grad_fn=<DivBackward0>) 0.81 0.022222222222222223\n",
      "1\n",
      "9 tensor(0.0907, grad_fn=<DivBackward0>) 0.79875 0.02\n",
      "3\n",
      "10 tensor(0.0863, grad_fn=<DivBackward0>) 0.7875 0.02727272727272727\n",
      "-3\n",
      "11 tensor(0.0842, grad_fn=<DivBackward0>) 0.7762500000000001 0.008333333333333333\n",
      "9\n",
      "12 tensor(0.0914, grad_fn=<DivBackward0>) 0.765 0.038461538461538464\n",
      "-3\n",
      "13 tensor(0.0918, grad_fn=<DivBackward0>) 0.75375 0.02857142857142857\n",
      "1\n",
      "14 tensor(0.0817, grad_fn=<DivBackward0>) 0.7424999999999999 0.03333333333333333\n",
      "1\n",
      "15 tensor(0.0976, grad_fn=<DivBackward0>) 0.7312500000000001 0.03125\n",
      "9\n",
      "16 tensor(0.0817, grad_fn=<DivBackward0>) 0.7200000000000001 0.052941176470588235\n",
      "3\n",
      "17 tensor(0.0902, grad_fn=<DivBackward0>) 0.70875 0.06111111111111111\n",
      "1\n",
      "18 tensor(0.0953, grad_fn=<DivBackward0>) 0.6975 0.06315789473684211\n",
      "-5\n",
      "19 tensor(0.0904, grad_fn=<DivBackward0>) 0.68625 0.05\n",
      "9\n",
      "20 tensor(0.0797, grad_fn=<DivBackward0>) 0.675 0.07142857142857142\n",
      "1\n",
      "21 tensor(0.0863, grad_fn=<DivBackward0>) 0.6637500000000001 0.06818181818181818\n",
      "5\n",
      "22 tensor(0.0772, grad_fn=<DivBackward0>) 0.6525 0.07391304347826087\n",
      "1\n",
      "23 tensor(0.0876, grad_fn=<DivBackward0>) 0.64125 0.07083333333333333\n",
      "15\n",
      "24 tensor(0.0749, grad_fn=<DivBackward0>) 0.63 0.096\n",
      "-3\n",
      "25 tensor(0.0769, grad_fn=<DivBackward0>) 0.61875 0.084\n",
      "1\n",
      "26 tensor(0.0723, grad_fn=<DivBackward0>) 0.6075 0.068\n",
      "7\n",
      "27 tensor(0.0804, grad_fn=<DivBackward0>) 0.59625 0.1\n",
      "3\n",
      "28 tensor(0.0953, grad_fn=<DivBackward0>) 0.5850000000000001 0.108\n",
      "9\n",
      "29 tensor(0.0786, grad_fn=<DivBackward0>) 0.57375 0.128\n",
      "-3\n",
      "30 tensor(0.0804, grad_fn=<DivBackward0>) 0.5625 0.124\n",
      "-5\n",
      "31 tensor(0.0893, grad_fn=<DivBackward0>) 0.55125 0.12\n",
      "1\n",
      "32 tensor(0.0929, grad_fn=<DivBackward0>) 0.54 0.112\n",
      "3\n",
      "33 tensor(0.0860, grad_fn=<DivBackward0>) 0.52875 0.112\n",
      "1\n",
      "34 tensor(0.0801, grad_fn=<DivBackward0>) 0.5175 0.112\n",
      "13\n",
      "35 tensor(0.0832, grad_fn=<DivBackward0>) 0.50625 0.136\n",
      "1\n",
      "36 tensor(0.0861, grad_fn=<DivBackward0>) 0.49500000000000005 0.148\n",
      "3\n",
      "37 tensor(0.0932, grad_fn=<DivBackward0>) 0.48375 0.136\n",
      "7\n",
      "38 tensor(0.0787, grad_fn=<DivBackward0>) 0.47250000000000003 0.156\n",
      "7\n",
      "39 tensor(0.0892, grad_fn=<DivBackward0>) 0.46125 0.168\n",
      "7\n",
      "40 tensor(0.0824, grad_fn=<DivBackward0>) 0.45 0.184\n",
      "3\n",
      "41 tensor(0.0926, grad_fn=<DivBackward0>) 0.43875 0.176\n",
      "3\n",
      "42 tensor(0.0963, grad_fn=<DivBackward0>) 0.4275 0.172\n",
      "3\n",
      "43 tensor(0.0937, grad_fn=<DivBackward0>) 0.41625 0.176\n",
      "1\n",
      "44 tensor(0.0860, grad_fn=<DivBackward0>) 0.405 0.188\n",
      "9\n",
      "45 tensor(0.0866, grad_fn=<DivBackward0>) 0.39375 0.188\n",
      "-5\n",
      "46 tensor(0.0883, grad_fn=<DivBackward0>) 0.3825 0.18\n",
      "1\n",
      "47 tensor(0.0854, grad_fn=<DivBackward0>) 0.37124999999999997 0.176\n",
      "5\n",
      "48 tensor(0.0888, grad_fn=<DivBackward0>) 0.36000000000000004 0.184\n",
      "11\n",
      "49 tensor(0.1044, grad_fn=<DivBackward0>) 0.34875 0.176\n",
      "13\n",
      "50 tensor(0.0747, grad_fn=<DivBackward0>) 0.3375 0.212\n",
      "11\n",
      "51 tensor(0.0939, grad_fn=<DivBackward0>) 0.32625 0.232\n",
      "11\n",
      "52 tensor(0.0831, grad_fn=<DivBackward0>) 0.315 0.236\n",
      "9\n",
      "53 tensor(0.0871, grad_fn=<DivBackward0>) 0.30375 0.252\n",
      "9\n",
      "54 tensor(0.0879, grad_fn=<DivBackward0>) 0.29250000000000004 0.252\n",
      "5\n",
      "55 tensor(0.0893, grad_fn=<DivBackward0>) 0.28125 0.272\n",
      "7\n",
      "56 tensor(0.0960, grad_fn=<DivBackward0>) 0.27 0.296\n",
      "7\n",
      "57 tensor(0.0722, grad_fn=<DivBackward0>) 0.25875 0.308\n",
      "5\n",
      "58 tensor(0.0812, grad_fn=<DivBackward0>) 0.24750000000000003 0.316\n",
      "5\n",
      "59 tensor(0.0868, grad_fn=<DivBackward0>) 0.23625000000000002 0.328\n",
      "5\n",
      "60 tensor(0.0891, grad_fn=<DivBackward0>) 0.225 0.308\n",
      "9\n",
      "61 tensor(0.0826, grad_fn=<DivBackward0>) 0.21375 0.32\n",
      "13\n",
      "62 tensor(0.0816, grad_fn=<DivBackward0>) 0.2025 0.344\n",
      "17\n",
      "63 tensor(0.0684, grad_fn=<DivBackward0>) 0.19125 0.364\n",
      "15\n",
      "64 tensor(0.0788, grad_fn=<DivBackward0>) 0.18000000000000002 0.38\n",
      "-1\n",
      "65 tensor(0.0805, grad_fn=<DivBackward0>) 0.16875 0.364\n",
      "11\n",
      "66 tensor(0.0890, grad_fn=<DivBackward0>) 0.1575 0.38\n",
      "11\n",
      "67 tensor(0.0822, grad_fn=<DivBackward0>) 0.14625000000000002 0.4\n",
      "5\n",
      "68 tensor(0.0927, grad_fn=<DivBackward0>) 0.135 0.4\n",
      "-1\n",
      "69 tensor(0.0868, grad_fn=<DivBackward0>) 0.12375000000000001 0.392\n",
      "13\n",
      "70 tensor(0.0930, grad_fn=<DivBackward0>) 0.1125 0.4\n",
      "5\n",
      "71 tensor(0.0949, grad_fn=<DivBackward0>) 0.10125 0.42\n",
      "13\n",
      "72 tensor(0.0769, grad_fn=<DivBackward0>) 0.09000000000000001 0.444\n",
      "15\n",
      "73 tensor(0.0890, grad_fn=<DivBackward0>) 0.07875 0.468\n",
      "13\n",
      "74 tensor(0.0858, grad_fn=<DivBackward0>) 0.0675 0.476\n",
      "19\n",
      "75 tensor(0.0805, grad_fn=<DivBackward0>) 0.05625 0.488\n",
      "19\n",
      "76 tensor(0.0881, grad_fn=<DivBackward0>) 0.045000000000000005 0.508\n",
      "17\n",
      "77 tensor(0.0882, grad_fn=<DivBackward0>) 0.03375 0.524\n",
      "3\n",
      "78 tensor(0.0834, grad_fn=<DivBackward0>) 0.022500000000000003 0.508\n",
      "3\n",
      "79 tensor(0.0755, grad_fn=<DivBackward0>) 0.011250000000000001 0.496\n",
      "3\n",
      "80 tensor(0.0797, grad_fn=<DivBackward0>) 0.0 0.488\n",
      "9\n",
      "81 tensor(0.0729, grad_fn=<DivBackward0>) 0 0.488\n",
      "5\n",
      "82 tensor(0.0756, grad_fn=<DivBackward0>) 0 0.484\n",
      "5\n",
      "83 tensor(0.0799, grad_fn=<DivBackward0>) 0 0.48\n",
      "-3\n",
      "84 tensor(0.0843, grad_fn=<DivBackward0>) 0 0.464\n",
      "-5\n",
      "85 tensor(0.0861, grad_fn=<DivBackward0>) 0 0.444\n",
      "19\n",
      "86 tensor(0.0745, grad_fn=<DivBackward0>) 0 0.468\n",
      "19\n",
      "87 tensor(0.0793, grad_fn=<DivBackward0>) 0 0.48\n",
      "5\n",
      "88 tensor(0.0713, grad_fn=<DivBackward0>) 0 0.452\n",
      "3\n",
      "89 tensor(0.0786, grad_fn=<DivBackward0>) 0 0.428\n",
      "-1\n",
      "90 tensor(0.0966, grad_fn=<DivBackward0>) 0 0.424\n",
      "-1\n",
      "91 tensor(0.0738, grad_fn=<DivBackward0>) 0 0.396\n",
      "1\n",
      "92 tensor(0.0852, grad_fn=<DivBackward0>) 0 0.372\n",
      "-5\n",
      "93 tensor(0.0713, grad_fn=<DivBackward0>) 0 0.352\n",
      "-5\n",
      "94 tensor(0.0949, grad_fn=<DivBackward0>) 0 0.344\n",
      "3\n",
      "95 tensor(0.0758, grad_fn=<DivBackward0>) 0 0.324\n",
      "19\n",
      "96 tensor(0.0838, grad_fn=<DivBackward0>) 0 0.352\n",
      "19\n",
      "97 tensor(0.0872, grad_fn=<DivBackward0>) 0 0.364\n",
      "19\n",
      "98 tensor(0.0784, grad_fn=<DivBackward0>) 0 0.372\n",
      "-19\n",
      "99 tensor(0.0853, grad_fn=<DivBackward0>) 0 0.304\n"
     ]
    }
   ],
   "source": [
    "def QLoss(experiences, q_network, target_network):\n",
    "    loss = 0\n",
    "\n",
    "    for exp in experiences:\n",
    "        state, action, reward, next_state = exp.state, exp.action, exp.reward, exp.next_state\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = torch.flatten(state, start_dim = 0, end_dim = 1) # double check dimensions\n",
    "\n",
    "        if reward == None:\n",
    "            next_state = torch.from_numpy(next_state).float()\n",
    "            next_state = torch.flatten(next_state, start_dim = 0, end_dim = 1) # double check dimensions\n",
    "            loss += (gamma * torch.max(target_network(next_state)) - q_network(state)[action]) ** 2\n",
    "        else: # next_state is None\n",
    "            loss += (reward - q_network(state)[action]) ** 2\n",
    "\n",
    "    loss /= len(experiences)\n",
    "    return loss\n",
    "\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True if seed > 0 else False\n",
    "\n",
    "    # Initialise replay memory D to capacity N\n",
    "    buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    # Initialize action-value function Q and target network\n",
    "    # TODO: Add extra agents to play against\n",
    "    q_network = DeepQNetworkConnect4().to(device)\n",
    "    target_network = DeepQNetworkConnect4().to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    optimiser = torch.optim.Adam(q_network.parameters(), learning_rate)\n",
    "\n",
    "    rewards = []\n",
    "    for iter in range(int(total_timesteps)):\n",
    "        for step in range(int(train_games)):\n",
    "            epsilon = calculate_epsilon(iter, epsilon_start, epsilon_finish, total_timesteps, exploration_fraction)\n",
    "            # Generate games and add experiences\n",
    "            g = Game()\n",
    "            # TODO: Why is the first player dominating? first_player is random, should be 50/50\n",
    "            g.playGame(agent1 = q_network, agent2 = q_network, epsilon = epsilon, first_player = random.randint(1, 2))\n",
    "            rewards.append(g.state)\n",
    "            for experience in g.experiences:\n",
    "                buffer.add(experience)\n",
    "\n",
    "        print(sum(rewards[-20:-1]))\n",
    "\n",
    "        batch = buffer.sample(min(len(buffer.buffer), ideal_batch_size))\n",
    "        # states and next states should be floats (same as the OUTPUT)\n",
    "        # brackets are required to turn generator into a list\n",
    "\n",
    "        # get loss\n",
    "        loss = QLoss(batch, q_network, target_network)\n",
    "        if len(rewards) > 500:\n",
    "            print(iter, loss, epsilon, sum(rewards[-500:])/500)\n",
    "        else:\n",
    "            print(iter, loss, epsilon, sum(rewards)/len(rewards))\n",
    "            \n",
    "        # backprop\n",
    "        with torch.no_grad():\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
